//
// nekRS User Defined File
//

#include <math.h>
#include "udf.hpp"

#include <vector>

static dfloat P_U0;
static dfloat P_V0;
static dfloat P_W0;

static dfloat P_A0;
static dfloat P_D0;

static dfloat P_OMEGA;
static dfloat P_AMP;

occa::kernel dpdxKernel; 
occa::kernel exactUVWPKernel; 
occa::kernel userMeshVelocityKernel;
occa::memory o_x0, o_y0, o_z0;
static std::map<dlong, std::vector<dlong>> receivingRankToElements;
static std::map<dlong, dlong> bufferStart; // in terms of elements
static std::vector<MPI_Request> requests;
static MPI_Comm receivingCommunicator;
static std::vector<dfloat> buffer;
static dfloat numFields;

void userq(nrs_t *nrs, dfloat time, occa::memory o_S, occa::memory o_FS)
{
  mesh_t *mesh = nrs->meshV; 
  cds_t *cds   = nrs->cds; 
  const dlong Nlocal = nrs->meshV->Nelements * nrs->meshV->Np;

  dpdxKernel(Nlocal, time, mesh->o_x, mesh->o_y, mesh->o_z, 0*cds->fieldOffset[0], o_FS);
  dpdxKernel(Nlocal, time, mesh->o_x, mesh->o_y, mesh->o_z, 1*cds->fieldOffset[0], o_FS);
}                                                   

void UDF_LoadKernels(occa::properties& kernelInfo)
{
  // called from all ranks, so MPI collectives are O.K.
  int maxRank = platform->comm.mpiRank;
  MPI_Allreduce(MPI_IN_PLACE, &maxRank, 1, MPI_INT, MPI_MAX, platform->comm.mpiComm);

  setupAide &options = platform->options;

  dfloat mue, rho;
  options.getArgs("VISCOSITY", mue);
  options.getArgs("DENSITY", rho); 
  kernelInfo["defines/p_nu"] = mue/rho;
  kernelInfo["defines/p_U0"] = P_U0;
  kernelInfo["defines/p_V0"] = P_V0;
  kernelInfo["defines/p_W0"] = P_W0;
  kernelInfo["defines/p_A"]  = P_A0 * M_PI;
  kernelInfo["defines/p_D"]  = P_D0 * M_PI;
  kernelInfo["defines/p_amp"] = P_AMP;
  kernelInfo["defines/p_omega"] = P_OMEGA;
  kernelInfo["defines/p_pi"] = M_PI;

  dpdxKernel = oudfBuildKernel(kernelInfo, "dpdx");
  exactUVWPKernel = oudfBuildKernel(kernelInfo, "exactUVWP"); 
  userMeshVelocityKernel = oudfBuildKernel(kernelInfo, "userMeshVelocity");
}

void UDF_Setup0(MPI_Comm comm, setupAide &options)
{
  platform->par->extract("casedata", "p_u0", P_U0);
  platform->par->extract("casedata", "p_v0", P_V0);
  platform->par->extract("casedata", "p_w0", P_W0);
  platform->par->extract("casedata", "p_a0", P_A0);
  platform->par->extract("casedata", "p_d0", P_D0);
  platform->par->extract("casedata", "p_omega", P_OMEGA);
  platform->par->extract("casedata", "p_amp", P_AMP);

  if (platform->options.compareArgs("BUILD ONLY", "FALSE")) {
    double* const nek_cb_scnrs = (double*) nek::ptr("cb_scnrs");
    nek_cb_scnrs[0] = P_U0;
    nek_cb_scnrs[1] = P_V0;
    nek_cb_scnrs[2] = P_W0;
    nek_cb_scnrs[3] = P_A0;
    nek_cb_scnrs[4] = P_D0;
    nek_cb_scnrs[5] = P_OMEGA;
    nek_cb_scnrs[6] = P_AMP;
  }
}

void UDF_Setup(nrs_t *nrs)
{
  const auto tStart = MPI_Wtime();
  mesh_t *mesh = nrs->meshV;
  udf.sEqnSource = &userq;
  if(platform->options.compareArgs("MOVING MESH", std::string("TRUE"))) {
    o_x0 = platform->device.malloc(mesh->Nlocal, sizeof(dfloat), mesh->o_x);
    o_y0 = platform->device.malloc(mesh->Nlocal, sizeof(dfloat), mesh->o_y);
    o_z0 = platform->device.malloc(mesh->Nlocal, sizeof(dfloat), mesh->o_z);
  }

  MPI_Group parentGroup; // group associated with parent communicator
  MPI_Group sendingGroup; // group associated with my communicator
  MPI_Group receivingGroup; // group associated with other communicator

  int rootParent = 0;
  int rootSending = -1; // relative to parent
  int rootReceiving = -1; // relative to parent

  int parentRank;
  MPI_Comm_rank(platform->comm.mpiCommParent, &parentRank);

  MPI_Comm_group(platform->comm.mpiComm, &sendingGroup);
  MPI_Comm_group(platform->comm.mpiCommParent, &parentGroup);

  MPI_Group_difference(parentGroup, sendingGroup, &receivingGroup);

  MPI_Comm_create(platform->comm.mpiCommParent, receivingGroup, &receivingCommunicator);

  if(platform->comm.mpiRank == 0){
    MPI_Comm_rank(platform->comm.mpiCommParent, &rootSending);
  }

  MPI_Allreduce(MPI_IN_PLACE, &rootReceiving, 1, MPI_INT, MPI_MAX, platform->comm.mpiCommParent);
  MPI_Allreduce(MPI_IN_PLACE, &rootSending, 1, MPI_INT, MPI_MAX, platform->comm.mpiCommParent);

  auto * mesh = nrs->meshV;
  auto numGlobalElements = mesh->Nelements;
  MPI_Allreduce(MPI_IN_PLACE, &numGlobalElements, 1, MPI_DLONG, MPI_SUM, platform->comm.mpiComm);
  
  // TODO: not scalable in terms of memory
  const auto INVALID = -std::numeric_limits<dlong>::max();
  std::vector<dlong> globalElements(numGlobalElements, INVALID);
  std::vector<dlong> myGlobalElements(mesh->Nelements, INVALID);
  std::map<dlong, dlong> globalToLocal;
  for(int e = 0; e < mesh->Nelements; ++e){
    auto ge = nek::lglel(e);
    globalToLocal[ge] = e;
    myGlobalElements[e] = ge;
  }

  // receive from other session
  MPI_Bcast(globalElements.data(), numGlobalElements, MPI_DLONG, rootReceiving, platform->comm.mpiCommParent);

  // determine which ranks I need to send to based on matching global elements
  for(auto&& eg : myGlobalElements){
    const auto receivingRank = globalElements[eg];
    receivingRankToElements[receivingRank].push_back(globalToLocal[eg]);
  }

  std::fill(globalElements.begin(), globalElements.end(), INVALID);

  for(auto && eg : myGlobalElements){
    globalElement[eg] = parentRank;
  }

  MPI_Reduce(MPI_IN_PLACE, globalElements.data(), numGlobalElements, MPI_DLONG, MPI_MAX, 0, platform->comm.mpiComm);

  // send to other session
  MPI_Bcast(globalElements.data(), numGlobalElements, MPI_DLONG, rootSending, platform->comm.mpiCommParent);

  // determine the starting index for each rank
  dlong bufferStart = 0;
  for(auto&& [rank, elements] : receivingRankToElements){
    bufferStart[rank] = bufferStart;
    bufferStart += elements.size();
  }

  requests.resize(receivingRankToElements.size());

  numFields = nrs->NVfields nrs->Nscalar + 1; // velocity, pressure, and scalars

  buffer.resize(numFields * bufferStart * mesh->Np);

  if(platform->comm.mpiRank == 0){
    std::cout << "udf_setup took " << MPI_Wtime() - tStart << " seconds" << std::endl;
  }
}

void packBuf(nrs_t* nrs)
{
  auto * mesh = nrs->meshV;
  for(auto&& [rank, elements] : receivingRankToElements){
    auto start = numFields * mesh->Np * bufferStart[rank];
    const auto nElem = elements.size();
    for(auto&& e : elements){
      for(int n = 0; n < mesh->Np; ++n){
        const auto id = e * mesh->Np + n;
        buffer[start + n + 0 * nElem * mesh->Np] = nrs->U[id + 0 * nrs->fieldOffset];
        buffer[start + n + 1 * nElem * mesh->Np] = nrs->U[id + 1 * nrs->fieldOffset];
        buffer[start + n + 2 * nElem * mesh->Np] = nrs->U[id + 2 * nrs->fieldOffset];
        buffer[start + n + 3 * nElem * mesh->Np] = nrs->P[id];
        if(nrs->cds){
          for(int is = 0; is < nrs->Nscalar; ++is){
            buffer[start + n + (4 + is) * nElem * mesh->Np] = nrs->cds->S[id + is * nrs->fieldOffset];
          }
        }
      }
    }
  }
}

void sendBuf(nrs_t* nrs)
{
  int parentRank;
  MPI_Comm_rank(platform->comm.mpiCommParent, &parentRank);
  auto * mesh = nrs->meshV;

  size_t ctr = 0;
  for(auto&& [rank, elements] : receivingRankToElements){
    auto start = numFields * mesh->Np * bufferStart[rank];
    auto count = numFields * mesh->Np * elements.size();
    MPI_Isend(
      (void*)(buffer.data() + start),
      count * sizeof(dfloat),
      MPI_UNSIGNED_CHAR,
      rank,
      parentRank, // use sending rank as tag
      platform->comm.mpiCommParent,
      &requests[ctr++]);
  }
}

void UDF_ExecuteStep(nrs_t *nrs, dfloat time, int tstep)
{
  mesh_t *mesh = nrs->meshV;
  cds_t *cds = nrs->cds;

  if (tstep <= 5) {
    exactUVWPKernel(mesh->Nlocal, time, mesh->o_x, mesh->o_y, mesh->o_z, nrs->fieldOffset, nrs->o_P, nrs->o_U);
    if (nrs->pSolver) 
      ellipticZeroMean(nrs->pSolver, nrs->o_P);
    if (nrs->Nscalar > 0) 
      cds->o_S.copyFrom(nrs->o_U, mesh->Nlocal*sizeof(dfloat), 0*cds->fieldOffset[0]*sizeof(dfloat));
    if (nrs->Nscalar > 1) 
    cds->o_S.copyFrom(nrs->o_U, mesh->Nlocal*sizeof(dfloat), 1*cds->fieldOffset[0]*sizeof(dfloat));
  }

  if(platform->options.compareArgs("MOVING MESH", std::string("TRUE")))
    userMeshVelocityKernel(
      mesh->Nlocal,
      nrs->fieldOffset,
      time,
      o_x0,
      o_y0,
      o_z0,
      mesh->o_U);

  static bool firstSend = true;
  bool send = tstep > 10;
  if (send && firstSend) {
    firstSend = false;
    // communicate simulation results with coupled session, if applicable
    if(!nrs->multiSession) return;
    nrs->o_U.copyTo(nrs->U);
    nrs->o_P.copyTo(nrs->P);
    nrs->cds->o_S.copyTo(nrs->cds->S);

    packBuf(nrs);
    sendBuf(nrs);
  }
}
