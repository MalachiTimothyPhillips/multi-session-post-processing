//
// nekRS User Defined File
//

#include <math.h>
#include "udf.hpp"
#include <vector>
#include <unordered_map>
#include <unordered_set>

static dfloat P_U0;
static dfloat P_V0;
static dfloat P_W0;

static dfloat P_A0;
static dfloat P_D0;

static dfloat P_OMEGA;
static dfloat P_AMP;
static std::map<dlong, std::vector<dlong>> sendingRankToElements;
static std::map<dlong, dlong> bufferStart; // in terms of elements
static std::vector<MPI_Request> requests;
static MPI_Comm sendingCommunicator;
static std::vector<dfloat> buffer;
static dlong numFields;
static dlong rootReceiving;
static dlong rootSending;

void UDF_LoadKernels(occa::properties& kernelInfo)
{
}

void UDF_Setup0(MPI_Comm comm, setupAide &options)
{
  platform->par->extract("casedata", "p_u0", P_U0);
  platform->par->extract("casedata", "p_v0", P_V0);
  platform->par->extract("casedata", "p_w0", P_W0);
  platform->par->extract("casedata", "p_a0", P_A0);
  platform->par->extract("casedata", "p_d0", P_D0);
  platform->par->extract("casedata", "p_omega", P_OMEGA);
  platform->par->extract("casedata", "p_amp", P_AMP);

  if (platform->options.compareArgs("BUILD ONLY", "FALSE")) {
    double* const nek_cb_scnrs = (double*) nek::ptr("cb_scnrs");
    nek_cb_scnrs[0] = P_U0;
    nek_cb_scnrs[1] = P_V0;
    nek_cb_scnrs[2] = P_W0;
    nek_cb_scnrs[3] = P_A0;
    nek_cb_scnrs[4] = P_D0;
    nek_cb_scnrs[5] = P_OMEGA;
    nek_cb_scnrs[6] = P_AMP;
  }
}

void UDF_Setup(nrs_t *nrs)
{
  const auto tStart = MPI_Wtime();

  MPI_Group parentGroup; // group associated with parent communicator
  MPI_Group sendingGroup; // group associated with other communicator
  MPI_Group receivingGroup; // group associated with my communicator

  int rootParent = 0;
  rootSending = -1; // relative to parent
  rootReceiving = -1; // relative to parent

  int parentRank;
  MPI_Comm_rank(platform->comm.mpiCommParent, &parentRank);

  MPI_Comm_group(platform->comm.mpiComm, &receivingGroup);
  MPI_Comm_group(platform->comm.mpiCommParent, &parentGroup);

  MPI_Group_difference(parentGroup, receivingGroup, &sendingGroup);

  MPI_Comm_create(platform->comm.mpiCommParent, sendingGroup, &sendingCommunicator);

  if(platform->comm.mpiRank == 0){
    MPI_Comm_rank(platform->comm.mpiCommParent, &rootReceiving);
  }

  MPI_Allreduce(MPI_IN_PLACE, &rootReceiving, 1, MPI_INT, MPI_MAX, platform->comm.mpiCommParent);
  MPI_Allreduce(MPI_IN_PLACE, &rootSending, 1, MPI_INT, MPI_MAX, platform->comm.mpiCommParent);

  auto * mesh = nrs->meshV;
  auto numGlobalElements = mesh->Nelements;
  MPI_Allreduce(MPI_IN_PLACE, &numGlobalElements, 1, MPI_DLONG, MPI_SUM, platform->comm.mpiComm);
  
  // TODO: not scalable in terms of memory
  const auto INVALID = -std::numeric_limits<dlong>::max();
  std::vector<dlong> globalElements(numGlobalElements, INVALID);
  std::vector<dlong> myGlobalElements(mesh->Nelements, INVALID);
  std::map<dlong, dlong> globalToLocal;
  for(int e = 0; e < mesh->Nelements; ++e){
    auto ge = nek::lglel(e);
    myGlobalElements[e] = ge;
    globalToLocal[ge] = e;
    globalElements[ge] = parentRank;
  }

  MPI_Allreduce(MPI_IN_PLACE, globalElements.data(), numGlobalElements, MPI_DLONG, MPI_MAX, platform->comm.mpiComm);

  // share with other session
  MPI_Bcast(globalElements.data(), numGlobalElements, MPI_DLONG, rootReceiving, platform->comm.mpiCommParent);

  // receive from the other session
  MPI_Bcast(globalElements.data(), numGlobalElements, MPI_DLONG, rootSending, platform->comm.mpiCommParent);

  // determine which ranks will send to me based on matching global elements
  for(auto&& eg : myGlobalElements){
    const auto sendingRank = globalElements[eg];
    sendingRankToElements[sendingRank].push_back(globalToLocal[eg]);
  }

  // determine the starting index for each rank
  dlong start = 0;
  for(auto&& [rank, elements] : sendingRankToElements){
    bufferStart[rank] = start;
    start += elements.size();
  }

  numFields = nrs->NVfields + nrs->Nscalar + 1; // velocity, pressure, and scalars

  buffer.resize(start*mesh->Np*numFields);
  requests.resize(sendingRankToElements.size());

  
  if(platform->comm.mpiRank == 0){
    std::cout << "udf_setup took " << MPI_Wtime() - tStart << " seconds" << std::endl;
  }

}

void recvBuf(nrs_t* nrs)
{
  int parentRank;
  MPI_Comm_rank(platform->comm.mpiCommParent, &parentRank);
  auto * mesh = nrs->meshV;

  size_t ctr = 0;
  for(auto&& [rank, elements] : sendingRankToElements){
    auto start = numFields * mesh->Np * bufferStart.at(rank);
    auto count = numFields * mesh->Np * elements.size();
    MPI_Irecv(
      (void*)(&buffer[start]),
      count * sizeof(dfloat),
      MPI_UNSIGNED_CHAR,
      rank,
      rank, // tag is sending rank
      platform->comm.mpiCommParent,
      &requests[ctr]);
    ctr++;
  }
}

void unpackBuf(nrs_t* nrs)
{
  auto * mesh = nrs->meshV;
  for(auto&& [rank, elements] : sendingRankToElements){
    auto eStart = bufferStart.at(rank);
    auto start = numFields * mesh->Np * eStart;
    const auto nElem = elements.size();
    int ctr = 0;
    for(auto&& e : elements){
      for(int n = 0; n < mesh->Np; ++n){
        const auto id = e * mesh->Np + n;
        const auto iid = ctr * mesh->Np + n;
        nrs->U[id + 0 * nrs->fieldOffset] = buffer[start + iid + 0 * nElem * mesh->Np];
        nrs->U[id + 1 * nrs->fieldOffset] = buffer[start + iid + 1 * nElem * mesh->Np];
        nrs->U[id + 2 * nrs->fieldOffset] = buffer[start + iid + 2 * nElem * mesh->Np];
        nrs->P[id]                        = buffer[start + iid + 3 * nElem * mesh->Np];
        if(nrs->cds){
          for(int is = 0; is < nrs->Nscalar; ++is){
            nrs->cds->S[id + is * nrs->fieldOffset] = buffer[start + iid + (4 + is) * nElem * mesh->Np];
          }
        }
      }
      ctr++;
    }
  }
}

void UDF_ExecuteStep(nrs_t *nrs, dfloat time, int tstep)
{
  auto * mesh = nrs->meshV;

  recvBuf(nrs);

  // do whatever work is possible without having the results here...
  MPI_Waitall(requests.size(), requests.data(), MPI_STATUSES_IGNORE);

  unpackBuf(nrs);
  
  nrs->o_U.copyFrom(nrs->U);
  nrs->o_P.copyFrom(nrs->P);
  if(nrs->cds){
    nrs->cds->o_S.copyFrom(nrs->cds->S);
  }

  // get information about timestep, time from other session
  dfloat otherTime;
  dlong othertstep;
  MPI_Bcast(&otherTime, 1, MPI_DFLOAT, 0, platform->comm.mpiCommParent);
  MPI_Bcast(&othertstep, 1, MPI_DLONG, 0, platform->comm.mpiCommParent);

  // do check sum on fields...
  auto metrics = [&](std::string name, dfloat* a){
    dfloat lmax = -std::numeric_limits<dfloat>::max();
    dfloat lmin = std::numeric_limits<dfloat>::max();
    dfloat lsum = 0.0;
    for(int i = 0; i < mesh->Nlocal; ++i){
      lmax = std::max(lmax, a[i]);
      lmin = std::min(lmin, a[i]);
      lsum += a[i];
    }

    MPI_Allreduce(MPI_IN_PLACE, &lmax, 1, MPI_DFLOAT, MPI_MAX, platform->comm.mpiComm);
    MPI_Allreduce(MPI_IN_PLACE, &lmin, 1, MPI_DFLOAT, MPI_MIN, platform->comm.mpiComm);
    MPI_Allreduce(MPI_IN_PLACE, &lsum, 1, MPI_DFLOAT, MPI_SUM, platform->comm.mpiComm);

    if(platform->comm.mpiRank == 0){
      std::cout << "max/min/sum " << name << " "
        << lmax << "/"
        << lmin << "/"
        << lsum << "\n";
    }
  };

  metrics("vx", nrs->U + 0 * nrs->fieldOffset);
  metrics("vy", nrs->U + 1 * nrs->fieldOffset);
  metrics("vz", nrs->U + 2 * nrs->fieldOffset);
  metrics("p", nrs->P);
  if(nrs->cds){
    for(int is = 0; is < nrs->Nscalar; ++is){
      metrics("s" + std::to_string(is), nrs->cds->S + is * nrs->fieldOffset);
    }
  }

  nek::ocopyToNek(otherTime, othertstep);
  nek::userchk();
}
